{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw data into a structured format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data is organized by folder named after fiscal years*. Each folder is composed of multiple xml files. Each xml file contains information about one award.\n",
    "There is no detailed description of the award data besides the xml tree composition. Some tags are self explanatory while others needed some investigation by throughout fully reading the NSF website.\n",
    "Here is a non-exhaustive list of tags and its description:\n",
    "\n",
    "1. **AwardTitle:** Title of award\n",
    " \n",
    "2. **AwardEffectiveDate:** Month,Day,Year when funding started\n",
    "\n",
    "3. **AwardExpirationDate:** Month,Day,Year when funding ended\n",
    "\n",
    "4. **AwardAmount:** Amount of money in USD awarded to date\n",
    "\n",
    "5. **AwardInstrument:** Award type (Standard Grant, Continuing Grant,...)\n",
    "\n",
    "6. **Organization:** NSF organization (Directorate and related Division) funding the grant\n",
    "\n",
    "7. **Investigator:** name of supervisor(s) (Principal Investigator, Co-Principal Investigator,...), contact info,...\n",
    "\n",
    "8. **Institution:** name of institution(s) receiving the award, phone number(s), address(es), \n",
    "\n",
    "9. **AwardID:** unique 7 digits identifiers of award\n",
    "\n",
    "*fiscal year Y starts October 1st,Y-1 and ends September 30th,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xml files are unstructured because there could be missing tags or new tags added over the years.\n",
    "From 1960 to 2017, there are about 450,000 awards which means as many files to read!\n",
    "\n",
    "Therefore one solution is to condense all that data into 2 CSV files. One containing \"short\" information (low byte size) and another one containing \"long\" information (Basically just ID and abstract) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Process2struct.png \" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Python modules:\n",
    "    * BeautifulSoup  \n",
    "    * nltk\n",
    "    * multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov  7 21:59:19 2017\n",
    "\n",
    "@author: herma\n",
    "\n",
    "FROM 1967 to 2017, total size zipped is 666-699 MB\n",
    "Unzipped it is 1.42 GB\n",
    "\"\"\"\n",
    "\n",
    "#import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# check for each tag and tag string\n",
    "def checkTag(tag):\n",
    "    # tag should be an element.Tag object\n",
    "    if tag is None or tag.string is None:\n",
    "        return None\n",
    "    else:\n",
    "        return tag.string.__str__()\n",
    "\n",
    "# have to cast to unicode via str() otherwise get recursion depth error\n",
    "# need to convert unicode back to beautifulSoup object\n",
    "        \n",
    "# sort out input_soup tags and return list of values\n",
    "def extract_Xml_Tag(input_xml):\n",
    "    \n",
    "    # make soup and extract tags\n",
    "    input_soup = BeautifulSoup(input_xml, 'lxml-xml')\n",
    "    # fetch tags\n",
    "    Title = checkTag(input_soup.AwardTitle)\n",
    "    Eff_date = checkTag(input_soup.AwardEffectiveDate)\n",
    "    Exp_date = checkTag(input_soup.AwardExpirationDate)\n",
    "    # Error checking on tags that needs casting\n",
    "    Amount = checkTag(input_soup.AwardAmount)\n",
    "    if input_soup.AwardInstrument is not None:\n",
    "        AwardInstr = checkTag(input_soup.AwardInstrument.Value)\n",
    "    else:\n",
    "        AwardInstr = None\n",
    "    if input_soup.Organization is not None:\n",
    "#        Org_code = checkTag(input_soup.Organization.Code)\n",
    "#        if Org_code is not None: Org_code = int(Org_code)\n",
    "        if input_soup.Organization.Division is not None:\n",
    "            Org_div = checkTag(input_soup.Organization.Division.LongName)\n",
    "        else:\n",
    "            Org_div = None\n",
    "        if input_soup.Organization.Directorate is not None:\n",
    "            Org_dir = checkTag(input_soup.Organization.Directorate.LongName)\n",
    "        else:\n",
    "            Org_dir = None\n",
    "    else:\n",
    "        Org_dir = None\n",
    "        Org_div = None\n",
    "    if input_soup.ProgramOfficer is not None:\n",
    "        NSF_officer = checkTag(input_soup.ProgramOfficer.SignBlockName)\n",
    "    else:\n",
    "        NSF_officer = None\n",
    "    Award_ID = checkTag(input_soup.AwardID)\n",
    "    if Award_ID is not None: Award_ID = int(Award_ID)\n",
    "    if input_soup.Investigator is not None:\n",
    "        PI_firstname = checkTag(input_soup.Investigator.FirstName)\n",
    "        PI_lastname = checkTag(input_soup.Investigator.LastName)\n",
    "#        if PI_firstname is not None and PI_lastname is not None:\n",
    "#            PI_Fullname = PI_firstname + ' ' + PI_lastname\n",
    "#        elif PI_firstname is None and PI_lastname is not None:\n",
    "#            PI_Fullname = PI_lastname\n",
    "#        else:\n",
    "#            PI_Fullname = None\n",
    "    else:\n",
    "#        PI_Fullname = None\n",
    "        PI_firstname = None\n",
    "        PI_lastname = None\n",
    "    if input_soup.Institution is not None:\n",
    "        Institution = checkTag(input_soup.Institution.Name)\n",
    "        Institution_state = checkTag(input_soup.Institution.StateCode)\n",
    "    else:\n",
    "        Institution = None\n",
    "        Institution_state = None\n",
    "    if input_soup.ProgramElement is not None:\n",
    "        Program_code = checkTag(input_soup.ProgramElement.Code)\n",
    "        Program_text = checkTag(input_soup.ProgramElement.Text)\n",
    "    else:\n",
    "        Program_code = None\n",
    "        Program_text = None\n",
    "    # compile all tags recipient into one list\n",
    "    tags = [ Title, Eff_date, Exp_date, Amount, AwardInstr,\\\n",
    "                Org_dir, Org_div, NSF_officer, Award_ID, PI_firstname, PI_lastname,\\\n",
    "               Institution, Institution_state, Program_code, Program_text ]\n",
    "    \n",
    "    # ABSTRACT ANALYSIS\n",
    "    # get abstract description\n",
    "    Abstract = checkTag(input_soup.AbstractNarration)\n",
    "    if Abstract is not None:\n",
    "        # Create tokens (create a list of words, ignores ponctuation)\n",
    "        tokens = tokenizer.tokenize(Abstract)\n",
    "        # set all words to lower case and remove stopwords and do stemming\n",
    "        sw = nltk.corpus.stopwords.words('english')\n",
    "        words = [stemmer.stem(w.lower()) for w in tokens if w.lower() not in sw]\n",
    "        # Create dictionary where key=word and value=count\n",
    "        dict_word_freq = nltk.FreqDist(words)\n",
    "        \n",
    "        # sort dict by values using get(), keep top 3 words in dataframe\n",
    "        # sorted makes a list of iterable from dict\n",
    "        Top3words = sorted(dict_word_freq, key=dict_word_freq.get,\\\n",
    "                           reverse=True)[:3]\n",
    "#        # add to tags\n",
    "#        tags.append('-'.join(Top3words))\n",
    "        \n",
    "        # appends tags and entire dictionary too\n",
    "        tags.extend(['-'.join(Top3words), Abstract])\n",
    "    else:\n",
    "        # append empty string if not available\n",
    "#        tags.append('')\n",
    "        tags.extend(['', ''])\n",
    "        \n",
    "    # make a list of information\n",
    "    return tags\n",
    "\n",
    "\n",
    "# read and file and extract info\n",
    "def readExtract(file_list):\n",
    "\n",
    "    Tag_listOflist = []\n",
    "    # read data in each xml file\n",
    "    for thisfname in file_list:\n",
    "        with open(thisfname, encoding='utf-8') as f:\n",
    "            xml_text = f.read()\n",
    "        \n",
    "        # extract info from xml\n",
    "        tag_list = extract_Xml_Tag( xml_text )\n",
    "        \n",
    "        # append list\n",
    "        Tag_listOflist.append(tag_list)\n",
    "        \n",
    "    return Tag_listOflist\n",
    "\n",
    "# Create tokenizer to use in loop\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "# use stemmer for abstract\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#### The Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # counter to update print statement showing progression\n",
    "    ListOfInfo = ['AwardTitle', 'AwardEffectiveDate', 'AwardExpirationDate', 'AwardAmount',\\\n",
    "                  'AwardInstrument',\\\n",
    "                'Directorate_Name', 'Division_Name', 'NSF_Officer_FullName', \\\n",
    "               'AwardID', 'PI_FirstName', 'PI_LastName',\\\n",
    "               'Institution_Name', 'Institution_State', \\\n",
    "                'ProgramElement_Code', 'ProgramElement_Text', 'Top3words']\n",
    "    \n",
    "    # year range for url, REMINDER: start at 1960\n",
    "    years = range(1960,2017+1)\n",
    "    # make sure csv file does not exist, otherwise delete it\n",
    "    CSV_DB_file = 'test.csv'\n",
    "    if os.path.isfile(CSV_DB_file): os.remove(CSV_DB_file)\n",
    "    CSV_Abstract_file = 'Abstract.csv'\n",
    "    if os.path.isfile(CSV_Abstract_file): os.remove(CSV_Abstract_file)\n",
    "    # create an empty dataframe\n",
    "#    df = pd.DataFrame(columns=ListOfInfo)\n",
    "    # number of processes (quad cores have 8 CPU, 1 CPU = 1 process at most)\n",
    "    NUM_PROCESS = 8\n",
    "    # cumulative number of files read\n",
    "    cumind=0\n",
    "    filecnt=0\n",
    "#    nck_counter=0\n",
    "    # get start time of timer for processing time\n",
    "    start_time = time.time()\n",
    "    # create pool\n",
    "    # sys.getrecursionlimit() returns max recursion (=2000 on my system)\n",
    "    # readExtract() exceeds that limit between 14 and 15 tasks\n",
    "    # Error is maximum recursion depth exceeded while calling a Python object\n",
    "    # sometimes error is maximum recursion depth exceeded while getting the str of an object\n",
    "    # another error maximum recursion depth exceeded in comparison\n",
    "    # is beautiful soup to blame for recursion? Yes! recursion is in BeautifulSoup\n",
    "    pool = multiprocessing.Pool(processes=NUM_PROCESS, maxtasksperchild=None)\n",
    "    for ny,y in enumerate(years):\n",
    "        # number of files read for current year\n",
    "        ind=0\n",
    "        # folders are organized by year\n",
    "        year_folder = '{}'.format(y)\n",
    "        listIn = glob.glob(os.path.join('NSF_data', year_folder, '*.xml'))\n",
    "        # use listIn as stack for multi-threading\n",
    "#        listIn = listIn[:1000]\n",
    "        # break down list in chunk of files, keep length of last list in listIn_ck\n",
    "        Nchunk = 200\n",
    "        listIn_ck = [ listIn[i:i+Nchunk] for i in range(0,len(listIn), Nchunk) ]\n",
    "        ck_adj = len(listIn_ck[-1])\n",
    "        tot_chunk = len(listIn_ck)\n",
    "        \n",
    "        \n",
    "        # feed pool with all files from current year\n",
    "        pool_outputs = pool.map(readExtract, listIn_ck)\n",
    "        # add tag list to dataframe\n",
    "        Unnested_listoflist = [ h[:-1] for l in pool_outputs for h in l ]\n",
    "        \n",
    "        # write select list to file \n",
    "        with open(CSV_DB_file, \"a\", newline='',  encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if ny == 0: writer.writerow(ListOfInfo)\n",
    "            writer.writerows(Unnested_listoflist)\n",
    "            \n",
    "        # take care of the abstract, keep award ID (index 7)\n",
    "        Abst_list = [ [ h[ListOfInfo.index('AwardID')] ,\\\n",
    "                       h[ListOfInfo.index('AwardEffectiveDate')], h[-1]] \\\n",
    "                     for l in pool_outputs for h in l ]\n",
    "        with open(CSV_Abstract_file, \"a\", newline='',  encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if ny == 0: writer.writerow(['AwardID', 'AwardEffectiveDate', 'Raw_Abstract'])\n",
    "            writer.writerows(Abst_list)\n",
    "        \n",
    "        # file counters\n",
    "        cumind += tot_chunk*Nchunk + ck_adj - Nchunk\n",
    "        ind += tot_chunk*Nchunk + ck_adj - Nchunk\n",
    "        \n",
    "        print('\\rYear {}, File #{:6d},Total File {:6d}'.format\\\n",
    "              (y,ind,cumind) ,end='', flush=True)\n",
    "        \n",
    "    # close pool\n",
    "    pool.close()\n",
    "    # make sure all processes are fisnished, map() does it too!\n",
    "    pool.join()\n",
    "    # closing print statement\n",
    "    print('\\rYear {}, File #{:6d},Total File {:6d}'.format(y,ind,cumind),\\\n",
    "                                                          end='\\n', flush=True)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    # First 1000 files in 2016 and 2017: ~ 6.3s, more than 2x than no pool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
