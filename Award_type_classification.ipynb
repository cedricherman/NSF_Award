{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Award Type Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/AwardType_rep.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of award:\n",
    "\n",
    "1. **Standard Grant** <br>\n",
    "    NSF provides a specific level of support for a specified period of time with\n",
    "    no statement of NSF intent to provide additional future support without\n",
    "    submission of another proposal.<br><br>\n",
    "\n",
    "2. **Continuing grant** <br>\n",
    "    NSF provides a specific level of support for an initial specified period of time,\n",
    "    usually a year, with a statement of intent to provide additional support of the\n",
    "    project for additional periods, provided funds are available and the results\n",
    "    achieved warrant further support.<br><br>\n",
    "\n",
    "3. **Fellowship** <br>\n",
    "    The purpose of the NSF Graduate Research Fellowship Program (GRFP) is to help\n",
    "    ensure the vitality and diversity of the scientific and engineering workforce\n",
    "    of the United States. The program recognizes and supports outstanding graduate\n",
    "    students who are pursuing research-based master's and doctoral degrees in science,\n",
    "    technology, engineering, and mathematics (STEM) or in STEM education. The GRFP\n",
    "    provides three years of support for the graduate education of individuals who \n",
    "    have demonstrated their potential for significant research achievements in STEM \n",
    "    or STEM education.<br><br>\n",
    "\n",
    "4. **Cooperative Agreement** <br>\n",
    "    A type of assistance award which should be used when substantial agency\n",
    "    involvement is anticipated during the project performance period. Substantial\n",
    "    agency involvement may be necessary when an activity is technically and/or\n",
    "    managerially complex and requires extensive or close coordination between NSF\n",
    "    and the awardee.<br><br>\n",
    "\n",
    "Three others type of award were discarded because of their rarity and/or their meaning remained a mystery (namely GAA, Intergovernmental Personnel Award and Personnel Agreement). All three combined they represent less than 0.05% of the data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Award Type based on Abstract\n",
    "\n",
    "The idea is to use bag of words or N-grams analyzer on Abstract and use them as features for a classification algorithm.\n",
    "Each abstract is preprocessed to remove remaining tags or web link. Stop words are also removed as well as first names. Lemmatization was added to remove plural forms of words.\n",
    "\n",
    "Scikit-learn was used to create a pipeline for vectorization (bag of words/N-grams) and classification algorithm (Multinomial Naive-Bayes). Note that vectorization was followed by tf-idf (term-frequency multiply by inverse document-frequency) to normalized counts and limit the effect of high frequency words.\n",
    "\n",
    "After optimization, an accuracy of 77% was achieved.\n",
    "Confusion matrix is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ConfusionMat_AwardType.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python module:\n",
    "  * utilsvectorizer (custom module for vectorization, Bag of words, N-grams)\n",
    "  * Abstract_transformation (custom  module to clean Abstract data)\n",
    "  * AwardInstr_transformation (custom  module to clean Award type data)\n",
    "  * sklearn (Multinomial Naive Bayes)\n",
    "  * nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov  8 23:52:41 2017\n",
    "\n",
    "@author: herma\n",
    "\"\"\"\n",
    "\n",
    "# import custom vectorizer and associated function\n",
    "from utils import utilsvectorizer\n",
    "\n",
    "# don't use use stopwords from nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#sw = stopwords.words('english')\n",
    "## sklearn stopword list is more extensive, ENGLISH_STOP_WORDS is the same\n",
    "## as stop_words='english' for CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "# add list of first names from nltk, ATTENTION names has duplicates!!! use union()\n",
    "# and each name starts with a CAPITAL LETTER\n",
    "from nltk.corpus import names\n",
    "firstname_corp = [na.lower() for na in names.words()]\n",
    "sw = stop_words.ENGLISH_STOP_WORDS.union(firstname_corp)\n",
    "\n",
    "\n",
    "# GET DATA\n",
    "#######-----------------------------------------------------------------#######\n",
    "from utils import Abstract_transformation as abt\n",
    "# get data set\n",
    "df_corpus = abt.get_Abstract('Abstract_full_Startdate.csv')\n",
    "#######-----------------------------------------------------------------#######\n",
    "from utils import AwardInstr_transformation as awt\n",
    "# get Target\n",
    "df_Award_Instr_target = awt.get_Award_Instrument('DB_1960_to_2017.csv')\n",
    "#######-----------------------------------------------------------------#######\n",
    "\n",
    "# MERGE\n",
    "#######-----------------------------------------------------------------#######\n",
    "import pandas as pd\n",
    "# merge corpus and target on AwardID. AwardID is conserved\n",
    "df = pd.merge(df_corpus, df_Award_Instr_target, how='inner', on=['AwardID'])\n",
    "#######-----------------------------------------------------------------#######\n",
    "# temporary downsizez of data\n",
    "#df = df.iloc[:10]\n",
    "\n",
    "# LABEL\n",
    "#######-----------------------------------------------------------------#######\n",
    "# label those categories\n",
    "target = df.AwardInstrument\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Award_Instr_encoder = LabelEncoder()\n",
    "Award_Instr_coded = Award_Instr_encoder.fit_transform(target)\n",
    "#######-----------------------------------------------------------------#######\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# divide data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split data/target in train and test sets\n",
    "corpus_train, corpus_test, target_train, target_test = train_test_split(\n",
    "                                                    df.Raw_Abstract, Award_Instr_coded,\\\n",
    "                                                     test_size=0.3, random_state=42)\n",
    "# retrieve target names\n",
    "target_train_names =  Award_Instr_encoder.inverse_transform(target_train)\n",
    "target_test_names =  Award_Instr_encoder.inverse_transform(target_test)\n",
    "target_names_list = Award_Instr_encoder.classes_\n",
    "\n",
    "################################################################################\n",
    "## Define pipeline\n",
    "## CountVectorizer--->tf-idf--->Naive Bayes\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "text_clf = Pipeline([('CustomVect', utilsvectorizer.CustomVectorizer(\\\n",
    "                              ngram_range=(3, 4),\\\n",
    "                              min_df = 1,\\\n",
    "                              max_df = 1.0,\\\n",
    "                              analyzer = 'word',\\\n",
    "                              stop_words = sw,\\\n",
    "                              strip_accents = 'unicode',\\\n",
    "                              token_pattern = r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',\\\n",
    "                        preprocessor = utilsvectorizer.remove_Tag_Http )),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                      ('clf', MultinomialNB(alpha=1e-2)),\n",
    "                         ])\n",
    "t0 = time()\n",
    "# train, get model named text_clf\n",
    "text_clf.fit(corpus_train, target_train) \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "# test\n",
    "predicted = text_clf.predict(corpus_test)\n",
    "print( 'Accuracy = {:.4f}'.format( np.mean(predicted == target_test) ) )\n",
    "# 69 %\n",
    "# more metrics\n",
    "from sklearn import metrics\n",
    "# precision, recall, f1 score\n",
    "print(metrics.classification_report(target_test,\\\n",
    "                                     predicted,\\\n",
    "                                     target_names = target_names_list))\n",
    "# confusion matrix\n",
    "mat = metrics.confusion_matrix(target_test, predicted)\n",
    "print(mat)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=target_names_list,\n",
    "            yticklabels=target_names_list)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "\n",
    "## save model\n",
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(text_clf, 'NB_default_Model.pkl')\n",
    "##text_clf = joblib.load('NB_default_Model.pkl') \n",
    "\n",
    "# ngram_range=(1, 2) ---> 77%\n",
    "\n",
    "# ngram_range=(2, 3) ---> 78%\n",
    "#Accuracy = 0.78\n",
    "#                       precision    recall  f1-score   support\n",
    "#\n",
    "#     Continuing grant       0.71      0.59      0.64     31671\n",
    "#Cooperative Agreement       0.72      0.27      0.40      1058\n",
    "#           Fellowship       0.95      0.61      0.74      1850\n",
    "#       Standard Grant       0.80      0.88      0.84     63765\n",
    "#\n",
    "#          avg / total       0.77      0.78      0.77     98344\n",
    "\n",
    "###############################################################################\n",
    "## Grid search\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#\n",
    "## ngram: used unigram (bag of word) or bigrams\n",
    "## use_idf: Enable inverse-document-frequency reweighting.\n",
    "## SGD alpha is the regularization constant\n",
    "## pick 3 param out of 2 choices each: 2^3 = 8 possibilities\n",
    "#parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#               'tfidf__use_idf': (True, False),\n",
    "#               'clf__alpha': (1e-2, 1e-3),\n",
    "#}\n",
    "#\n",
    "#gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "#gs_clf = gs_clf.fit(corpus_train, target_train)\n",
    "#\n",
    "#print(gs_clf.best_score_)\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "## import to pandas to see it\n",
    "#gs_clf.cv_results_\n",
    "#\n",
    "## save model\n",
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(text_clf, 'SVM_default_Model.pkl')\n",
    "##text_clf = joblib.load('SVM_default_Model.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
